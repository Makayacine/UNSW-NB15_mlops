{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "220e1648-23d9-4247-a5ab-ffb89cc0b83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/02 23:14:40 INFO mlflow.tracking.fluent: Experiment with name 'unsw-nb15' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 1 — Setup / Config =====\n",
    "import json, gc, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "import torch\n",
    "import mlflow\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- MLflow ----\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "EXPERIMENT_NAME = \"unsw-nb15\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# ---- Device ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# ---- Quick diagnostics helper ----\n",
    "def gpu_cpu_status(tag=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        used = torch.cuda.memory_allocated(0) / 1024**2\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**2\n",
    "        print(f\"{tag} GPU 0 | Mem Used: {used:.1f}MB / {total:.1f}MB\")\n",
    "    print(f\"{tag} CPU Mem Used: {psutil.virtual_memory().percent}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e3e479-7b31-42bc-ae62-d3a2195076ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:\n",
      "  train_df: (96822, 36)  -> UNSW_NB15_train_clean.parquet\n",
      "  test_df : (82332, 36)   -> UNSW_NB15_test_clean.parquet\n",
      "\n",
      "Label distribution (train):\n",
      "label\n",
      "0    48894\n",
      "1    47928\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2b — Load cleaned parquet sitting next to this notebook =====\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "BASE = Path.cwd()\n",
    "TRAIN_PQ = BASE / \"UNSW_NB15_train_clean.parquet\"\n",
    "TEST_PQ  = BASE / \"UNSW_NB15_test_clean.parquet\"\n",
    "\n",
    "assert TRAIN_PQ.exists(), f\"Missing file: {TRAIN_PQ}\"\n",
    "assert TEST_PQ.exists(),  f\"Missing file: {TEST_PQ}\"\n",
    "\n",
    "train_df = pd.read_parquet(TRAIN_PQ)\n",
    "test_df  = pd.read_parquet(TEST_PQ)\n",
    "\n",
    "print(\"Loaded:\")\n",
    "print(f\"  train_df: {train_df.shape}  -> {TRAIN_PQ.name}\")\n",
    "print(f\"  test_df : {test_df.shape}   -> {TEST_PQ.name}\")\n",
    "\n",
    "# quick sanity checks\n",
    "assert \"label\" in train_df.columns, \"train_df missing 'label'\"\n",
    "assert \"label\" in test_df.columns,  \"test_df missing 'label'\"\n",
    "print(\"\\nLabel distribution (train):\")\n",
    "print(train_df[\"label\"].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ec137a4-5987-4023-a410-78b856e05ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 34 features. 'attack_cat' has been excluded.\n",
      "------------------------------\n",
      "Features have been scaled using StandardScaler.\n",
      "------------------------------\n",
      "device: cuda\n",
      "feature_cols: 34 features\n",
      "train / val / test shapes: (82298, 34) / (14524, 34) / (82332, 34)\n",
      "y_train pos rate: 0.4950  -> pos_weight=1.020\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 3 (Final Corrected) — Prep, Scale, and Load =====\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "TARGET = \"label\"\n",
    "# --- MODIFIED: Exclude 'attack_cat' from the feature list ---\n",
    "feature_cols = [c for c in train_df.columns if c not in [TARGET, \"attack_cat\"]]\n",
    "print(f\"Using {len(feature_cols)} features. 'attack_cat' has been excluded.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# If anything is still non-numeric, convert to category codes\n",
    "non_numeric = [c for c in feature_cols if not np.issubdtype(train_df[c].dtype, np.number)]\n",
    "if non_numeric:\n",
    "    print(\"Converting non-numeric columns to categorical codes:\", non_numeric)\n",
    "    for c in non_numeric:\n",
    "        cats = pd.Categorical(train_df[c]).categories\n",
    "        train_df[c] = pd.Categorical(train_df[c], categories=cats).codes\n",
    "        test_df[c]  = pd.Categorical(test_df[c], categories=cats).codes\n",
    "\n",
    "# Step 1: Feature matrices / targets\n",
    "X      = train_df[feature_cols].astype(\"float32\").values\n",
    "y      = train_df[TARGET].astype(\"float32\").values\n",
    "X_test = test_df[feature_cols].astype(\"float32\").values\n",
    "y_test = test_df[TARGET].astype(\"float32\").values\n",
    "\n",
    "# Step 2: Train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y, shuffle=True\n",
    ")\n",
    "\n",
    "# Step 3: Scale Features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "X_test  = scaler.transform(X_test)\n",
    "print(\"Features have been scaled using StandardScaler.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# Step 4: Create Tensors from the scaled data\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_t   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Step 5: Create DataLoaders\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 8192\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train_t, y_train_t),\n",
    "    batch_size=BATCH_SIZE, shuffle=True,\n",
    "    pin_memory=(device.type == \"cuda\"), num_workers=0\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(X_val_t, y_val_t),\n",
    "    batch_size=BATCH_SIZE, shuffle=False,\n",
    "    pin_memory=(device.type == \"cuda\"), num_workers=0\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test_t, y_test_t),\n",
    "    batch_size=BATCH_SIZE, shuffle=False,\n",
    "    pin_memory=(device.type == \"cuda\"), num_workers=0\n",
    ")\n",
    "\n",
    "# Step 6: Re-calculate class imbalance weight\n",
    "pos = y_train.sum()\n",
    "neg = len(y_train) - pos\n",
    "pos_weight_val = torch.tensor(neg / max(pos, 1.0), dtype=torch.float32, device=device)\n",
    "\n",
    "# Final diagnostics\n",
    "print(f\"device: {device}\")\n",
    "print(f\"feature_cols: {len(feature_cols)} features\")\n",
    "print(f\"train / val / test shapes: {X_train.shape} / {X_val.shape} / {X_test.shape}\")\n",
    "print(f\"y_train pos rate: {pos/len(y_train):.4f}  -> pos_weight={pos_weight_val.item():.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0267d43b-9cd0-4b03-ad97-22b380d8a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4 — training helpers with progress + diagnostics =====\n",
    "import json, math, time, pathlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tqdm (status bar)\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    class tqdm:\n",
    "        def __init__(self, it, **kwargs): self.it = it\n",
    "        def __iter__(self): return iter(self.it)\n",
    "        def set_postfix(self, **kwargs): pass\n",
    "        def update(self, *args, **kwargs): pass\n",
    "        def close(self): pass\n",
    "\n",
    "# Optional CPU mem readout\n",
    "try:\n",
    "    import psutil\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gpu_cpu_diagnostics():\n",
    "    \"\"\"Return a dict with GPU and CPU memory diagnostics (safe if unavailable).\"\"\"\n",
    "    diag = {}\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            free_b, total_b = torch.cuda.mem_get_info()\n",
    "            used_b = total_b - free_b\n",
    "            diag[\"gpu_used_mb\"] = round(used_b / (1024**2), 1)\n",
    "            diag[\"gpu_total_mb\"] = round(total_b / (1024**2), 1)\n",
    "        except Exception:\n",
    "            diag[\"gpu_used_mb\"] = diag[\"gpu_total_mb\"] = None\n",
    "    else:\n",
    "        diag[\"gpu_used_mb\"] = diag[\"gpu_total_mb\"] = None\n",
    "\n",
    "    if psutil is not None:\n",
    "        try:\n",
    "            vm = psutil.virtual_memory()\n",
    "            diag[\"cpu_mem_percent\"] = vm.percent\n",
    "        except Exception:\n",
    "            diag[\"cpu_mem_percent\"] = None\n",
    "    else:\n",
    "        diag[\"cpu_mem_percent\"] = None\n",
    "    return diag\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    n = 0\n",
    "    bar = tqdm(loader, total=len(loader), leave=False)\n",
    "    for xb, yb in bar:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = xb.size(0)\n",
    "        running += loss.item() * bs\n",
    "        n += bs\n",
    "        bar.set_postfix(loss=f\"{running/max(n,1):.4f}\")\n",
    "    bar.close()\n",
    "    return running / max(n, 1)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_proba_torch(model, loader):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    for xb, _ in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        logits = model(xb)\n",
    "        p = torch.sigmoid(logits).detach().cpu().numpy().ravel()\n",
    "        probs.append(p)\n",
    "    return np.concatenate(probs, axis=0)\n",
    "\n",
    "def compute_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    out = {}\n",
    "    # Some splits can be single-class — guard ROC-AUC\n",
    "    try:\n",
    "        out[\"roc_auc\"] = float(roc_auc_score(y_true, y_prob))\n",
    "    except Exception:\n",
    "        out[\"roc_auc\"] = float(\"nan\")\n",
    "    out[\"accuracy\"]  = float(accuracy_score(y_true, y_pred))\n",
    "    out[\"f1\"]        = float(f1_score(y_true, y_pred, zero_division=0))\n",
    "    out[\"precision\"] = float(precision_score(y_true, y_pred, zero_division=0))\n",
    "    out[\"recall\"]    = float(recall_score(y_true, y_pred, zero_division=0))\n",
    "    return out\n",
    "\n",
    "def save_confusion_png(y_true, y_prob, path=\"confusion.png\", threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    fig.tight_layout()\n",
    "    pathlib.Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(path, dpi=140)\n",
    "    plt.close(fig)\n",
    "    return path\n",
    "\n",
    "def train_torch_model(model, train_loader, val_loader, epochs=8, lr=1e-3, pos_weight=None, mlflow_run=None):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) if pos_weight is not None else nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer)\n",
    "        y_val_prob = predict_proba_torch(model, val_loader)\n",
    "        # Extract ground truth from val_loader\n",
    "        y_val_true = torch.cat([y for _, y in val_loader.dataset], dim=0).cpu().numpy().ravel()\n",
    "        val_metrics = compute_metrics(y_val_true, y_val_prob)\n",
    "\n",
    "        diag = gpu_cpu_diagnostics()\n",
    "        msg = (\n",
    "            f\"Epoch {epoch}/{epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | \"\n",
    "            f\"Val F1: {val_metrics['f1']:.4f} | Val AUC: {val_metrics['roc_auc']:.4f} | \"\n",
    "            f\"GPU {('on' if torch.cuda.is_available() else 'off')} \"\n",
    "            f\"| Mem Used: {diag.get('gpu_used_mb')}MB / {diag.get('gpu_total_mb')}MB \"\n",
    "            f\"| CPU: {diag.get('cpu_mem_percent')}%\"\n",
    "        )\n",
    "        print(msg)\n",
    "\n",
    "        # Log to MLflow if a run is open\n",
    "        if mlflow_run is not None:\n",
    "            import mlflow\n",
    "            mlflow.log_metric(\"train_loss\", float(train_loss), step=epoch)\n",
    "            for k, v in val_metrics.items():\n",
    "                mlflow.log_metric(f\"val_{k}\", float(v), step=epoch)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99abf5a-e993-4732-96d5-1d3e1de79353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b253d2ba9b64ae7b894d605323e63df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8 | Train Loss: 0.7442 | Val F1: 0.4747 | Val AUC: 0.5132 | GPU on | Mem Used: 1189.5MB / 8187.5MB | CPU: 85.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7283c113f94dabb9d2d98d31253568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8 | Train Loss: 0.7198 | Val F1: 0.5452 | Val AUC: 0.5834 | GPU on | Mem Used: 1189.5MB / 8187.5MB | CPU: 85.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf434b15d5f4dc5be7ebfd970afbb2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8 | Train Loss: 0.6975 | Val F1: 0.6109 | Val AUC: 0.6562 | GPU on | Mem Used: 1189.5MB / 8187.5MB | CPU: 85.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17de06d0e158471aac085df0121d44ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8 | Train Loss: 0.6774 | Val F1: 0.6901 | Val AUC: 0.7264 | GPU on | Mem Used: 1189.5MB / 8187.5MB | CPU: 85.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3a67d3de96423ea1952922b8f824af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8 | Train Loss: 0.6596 | Val F1: 0.7231 | Val AUC: 0.7770 | GPU on | Mem Used: 1189.5MB / 8187.5MB | CPU: 85.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef93ac1c815a4e7485a77b3ff984d998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8 | Train Loss: 0.6436 | Val F1: 0.7715 | Val AUC: 0.8028 | GPU on | Mem Used: 1189.5MB / 8187.5MB | CPU: 85.8%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53507e06546241a893f8741f7eba907f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8 | Train Loss: 0.6296 | Val F1: 0.7809 | Val AUC: 0.8149 | GPU on | Mem Used: 1189.5MB / 8187.5MB | CPU: 85.7%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95051ce5c8d4f9eb813f7b758f5a78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8 | Train Loss: 0.6169 | Val F1: 0.7890 | Val AUC: 0.8223 | GPU on | Mem Used: 1189.5MB / 8187.5MB | CPU: 85.8%\n",
      "🏃 View run logreg_torch_gpu at: http://127.0.0.1:5000/#/experiments/1/runs/0bd5dc38957c450c9d9c3d6afcb487ba\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "✅ training complete, model still in memory\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 5 — Train Torch logistic regression (GPU) =====\n",
    "import torch.nn as nn, torch\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "pos_weight_t = torch.tensor([pos_weight_val], device=device)\n",
    "\n",
    "# simple logistic regression layer\n",
    "model = nn.Sequential(nn.Linear(n_features, 1))\n",
    "\n",
    "with mlflow.start_run(run_name=\"logreg_torch_gpu\") as run:\n",
    "    mlflow.log_param(\"model\", \"LogisticRegression_Torch\")\n",
    "    mlflow.log_param(\"epochs\", 8)\n",
    "    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "    mlflow.log_param(\"lr\", 1e-3)\n",
    "    mlflow.log_param(\"pos_weight\", float(pos_weight_val))\n",
    "    mlflow.log_param(\"n_features\", int(n_features))\n",
    "\n",
    "    # --- train (uses helper from Cell 4)\n",
    "    model = train_torch_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        epochs=8,\n",
    "        lr=1e-3,\n",
    "        pos_weight=pos_weight_t,\n",
    "        mlflow_run=mlflow.active_run()\n",
    "    )\n",
    "\n",
    "print(\"✅ training complete, model still in memory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2919a2a-33ad-4d9c-9e73-d62f00655279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ XGBoost trained. use_gpu=True | best_iteration=376\n",
      "VAL metrics: {'roc_auc': 0.9845176864058003, 'accuracy': 0.9261222803635362, 'f1': 0.9270613826388416, 'precision': 0.9066613482249701, 'recall': 0.9484005563282336}\n",
      "🏃 View run xgboost_gpu at: http://127.0.0.1:5000/#/experiments/1/runs/519388586ded4e2c91578229afac293b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 7 (compatible with your XGBoost version) =====\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "use_gpu   = (device.type == \"cuda\")\n",
    "tree_meth = \"gpu_hist\" if use_gpu else \"hist\"\n",
    "predictor = \"gpu_predictor\" if use_gpu else \"auto\"\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": 6,\n",
    "    \"eta\": 0.08,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"lambda\": 1.0,\n",
    "    \"min_child_weight\": 1.0,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"tree_method\": tree_meth,\n",
    "    \"predictor\": predictor,\n",
    "    \"scale_pos_weight\": float(pos_weight_val.item() if hasattr(pos_weight_val, \"item\") else pos_weight_val),\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Wrap in DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval   = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "with mlflow.start_run(run_name=\"xgboost_gpu\") as run:\n",
    "    mlflow.log_param(\"model\", \"XGBoost\")\n",
    "    mlflow.log_param(\"use_gpu\", use_gpu)\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    evals = [(dtrain, \"train\"), (dval, \"val\")]\n",
    "    xgb_model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=400,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Validation metrics\n",
    "    val_prob = xgb_model.predict(dval, iteration_range=(0, xgb_model.best_iteration+1))\n",
    "    val_metrics = compute_metrics(y_val, val_prob)   # from Cell 4\n",
    "\n",
    "    for k, v in val_metrics.items():\n",
    "        mlflow.log_metric(f\"val_{k}\", float(v))\n",
    "\n",
    "    print(f\"✅ XGBoost trained. use_gpu={use_gpu} | best_iteration={xgb_model.best_iteration}\")\n",
    "    print(\"VAL metrics:\", val_metrics)\n",
    "\n",
    "# keep xgb_model in memory for test eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1e8bbc6-4ea4-48bb-a22b-b76bd09e4c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4578cc60e1c4cb0a41f5c080a028445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 0.6531 | Val F1: 0.8522 | Val AUC: 0.8544 | GPU on | Mem Used: 1231.5MB / 8187.5MB | CPU: 86.5%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70850ab6686244ecbab17b72121a955f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train Loss: 0.5457 | Val F1: 0.8619 | Val AUC: 0.8720 | GPU on | Mem Used: 1231.5MB / 8187.5MB | CPU: 86.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e540cc7b0f5d4f21aed1e76bc7e3718d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train Loss: 0.4494 | Val F1: 0.8607 | Val AUC: 0.8903 | GPU on | Mem Used: 1231.5MB / 8187.5MB | CPU: 86.4%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847915735e0f4126aac7b3e63bfb1714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train Loss: 0.3937 | Val F1: 0.8702 | Val AUC: 0.9066 | GPU on | Mem Used: 1231.5MB / 8187.5MB | CPU: 86.5%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93537ca6838d4b4a898672cbc37460da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train Loss: 0.3556 | Val F1: 0.8878 | Val AUC: 0.9216 | GPU on | Mem Used: 1231.5MB / 8187.5MB | CPU: 86.6%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd9fcbd5c644f479e2c25d2147b12e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train Loss: 0.3275 | Val F1: 0.8935 | Val AUC: 0.9321 | GPU on | Mem Used: 1231.5MB / 8187.5MB | CPU: 86.6%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693ad7d790f2497e85a692c63dddadbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train Loss: 0.3061 | Val F1: 0.8982 | Val AUC: 0.9409 | GPU on | Mem Used: 1231.5MB / 8187.5MB | CPU: 87.1%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09667ef53de4b98987cd9a0468b0a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train Loss: 0.2906 | Val F1: 0.9006 | Val AUC: 0.9442 | GPU on | Mem Used: 1231.5MB / 8187.5MB | CPU: 87.1%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4e7f35b7a54f9097dd7978572b908b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train Loss: 0.2794 | Val F1: 0.9009 | Val AUC: 0.9478 | GPU on | Mem Used: 1231.5MB / 8187.5MB | CPU: 87.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f947968cce483f836e0e53f3dc0ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train Loss: 0.2728 | Val F1: 0.9009 | Val AUC: 0.9492 | GPU on | Mem Used: 1231.5MB / 8187.5MB | CPU: 87.0%\n",
      "✅ MLP trained. Run ID: 65fe0261d59744a19a26ecc5b4f6886a\n",
      "VAL: {'roc_auc': 0.9492410128602546, 'accuracy': 0.8940374552464886, 'f1': 0.9009333762471837, 'precision': 0.8385859796285201, 'recall': 0.9732962447844228}\n",
      "TEST: {'roc_auc': 0.9280904590397334, 'accuracy': 0.7988024097556236, 'f1': 0.8429545217531452, 'precision': 0.7391391091825029, 'recall': 0.980697961704756}\n",
      "🏃 View runs at: http://127.0.0.1:5000/#/experiments\n",
      "🏃 View run mlp_torch_gpu at: http://127.0.0.1:5000/#/experiments/1/runs/65fe0261d59744a19a26ecc5b4f6886a\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 9 — MLP (Neural Net) on GPU, log metrics only (no artifacts) =====\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import mlflow\n",
    "\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(n_features, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(64, 1),\n",
    ")\n",
    "\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "\n",
    "with mlflow.start_run(run_name=\"mlp_torch_gpu\") as run:\n",
    "    mlflow.log_params({\n",
    "        \"model\": \"MLP_Torch\",\n",
    "        \"device\": str(device),\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"lr\": LR,\n",
    "        \"pos_weight\": float(pos_weight_val.item() if hasattr(pos_weight_val, \"item\") else pos_weight_val),\n",
    "        \"n_features\": int(n_features),\n",
    "        \"hidden1\": 128,\n",
    "        \"hidden2\": 64,\n",
    "        \"dropout\": 0.2,\n",
    "    })\n",
    "\n",
    "    # Train (uses helpers from earlier cells)\n",
    "    mlp = train_torch_model(\n",
    "        mlp, train_loader, val_loader,\n",
    "        epochs=EPOCHS, lr=LR,\n",
    "        pos_weight=pos_weight_val,\n",
    "        mlflow_run=mlflow.active_run()\n",
    "    )\n",
    "\n",
    "    # Validate & Test\n",
    "    val_prob  = predict_proba_torch(mlp, val_loader)\n",
    "    test_prob = predict_proba_torch(mlp, test_loader)\n",
    "    val_metrics  = compute_metrics(y_val,  val_prob)\n",
    "    test_metrics = compute_metrics(y_test, test_prob)\n",
    "\n",
    "    for k, v in val_metrics.items():\n",
    "        mlflow.log_metric(f\"val_{k}\", float(v))\n",
    "    for k, v in test_metrics.items():\n",
    "        mlflow.log_metric(f\"test_{k}\", float(v))\n",
    "\n",
    "    print(f\"✅ MLP trained. Run ID: {run.info.run_id}\")\n",
    "    print(\"VAL:\", val_metrics)\n",
    "    print(\"TEST:\", test_metrics)\n",
    "    print(\"🏃 View runs at: http://127.0.0.1:5000/#/experiments\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e84e79-a4c5-4159-b56c-201fc8f1d119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomForestClassifier...\n",
      "✅ RandomForest trained.\n",
      "\n",
      "VAL metrics: {'roc_auc': 0.9821188432863417, 'accuracy': 0.9219911870008263, 'f1': 0.9230246620014947, 'precision': 0.9022446540045159, 'recall': 0.9447844228094576}\n",
      "🏃 View run random_forest_cpu at: http://127.0.0.1:5000/#/experiments/1/runs/ed7d75d4064a4632b3b7a99c72d4b0e7\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8 — Train Random Forest Classifier =====\n",
    "import mlflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "with mlflow.start_run(run_name=\"random_forest_cpu\") as run:\n",
    "    # --- Model setup ---\n",
    "    # Using class_weight='balanced' helps with any residual class imbalance.\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    print(\"Training RandomForestClassifier...\")\n",
    "\n",
    "    mlflow.log_param(\"model\", \"RandomForestClassifier\")\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"class_weight\", \"balanced\")\n",
    "\n",
    "    # --- Train the model ---\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    print(\"✅ RandomForest trained.\")\n",
    "\n",
    "    # --- Get validation metrics ---\n",
    "    # We need the probability of the positive class (1) for ROC AUC\n",
    "    val_prob = rf_model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Use the helper function from Cell 4 to calculate metrics\n",
    "    val_metrics = compute_metrics(y_val, val_prob)\n",
    "\n",
    "    # Log metrics to MLflow\n",
    "    for k, v in val_metrics.items():\n",
    "        mlflow.log_metric(f\"val_{k}\", float(v))\n",
    "\n",
    "    print(\"\\nVAL metrics:\", val_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c176e17-6400-4af9-a338-2ba0c17a47a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>val_best_thr</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.979767</td>\n",
       "      <td>0.852378</td>\n",
       "      <td>0.879728</td>\n",
       "      <td>0.797710</td>\n",
       "      <td>0.980544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.975089</td>\n",
       "      <td>0.865994</td>\n",
       "      <td>0.888784</td>\n",
       "      <td>0.818344</td>\n",
       "      <td>0.972492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP_Torch</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.928090</td>\n",
       "      <td>0.790555</td>\n",
       "      <td>0.838835</td>\n",
       "      <td>0.727750</td>\n",
       "      <td>0.989941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogReg_Torch</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.715268</td>\n",
       "      <td>0.692100</td>\n",
       "      <td>0.774084</td>\n",
       "      <td>0.649391</td>\n",
       "      <td>0.958043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model  val_best_thr   roc_auc  accuracy        f1  precision  \\\n",
       "1       XGBoost          0.45  0.979767  0.852378  0.879728   0.797710   \n",
       "3  RandomForest          0.50  0.975089  0.865994  0.888784   0.818344   \n",
       "2     MLP_Torch          0.35  0.928090  0.790555  0.838835   0.727750   \n",
       "0  LogReg_Torch          0.45  0.715268  0.692100  0.774084   0.649391   \n",
       "\n",
       "     recall  \n",
       "1  0.980544  \n",
       "3  0.972492  \n",
       "2  0.989941  \n",
       "0  0.958043  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 10 — Compare all models on TEST (with per-model best thresholds) =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def best_threshold(y_true, y_prob):\n",
    "    \"\"\"Grid-search threshold in [0,1] to maximize F1.\"\"\"\n",
    "    ts = np.linspace(0.05, 0.95, 19)\n",
    "    best_t, best_f1 = 0.5, -1.0\n",
    "    for t in ts:\n",
    "        f1 = compute_metrics(y_true, (y_prob >= t).astype(int))[\"f1\"]\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return float(best_t), float(best_f1)\n",
    "\n",
    "rows = []\n",
    "\n",
    "# ----- Logistic (torch) if present -----\n",
    "if \"model\" in globals():\n",
    "    val_prob_log = predict_proba_torch(model, val_loader)\n",
    "    t_log, _ = best_threshold(y_val, val_prob_log)\n",
    "\n",
    "    test_prob_log = predict_proba_torch(model, test_loader)\n",
    "    m_log = compute_metrics(y_test, test_prob_log, threshold=t_log)\n",
    "    rows.append({\n",
    "        \"model\": \"LogReg_Torch\",\n",
    "        \"val_best_thr\": t_log,\n",
    "        **m_log\n",
    "    })\n",
    "else:\n",
    "    print(\"↪️ Skipping logistic: variable `model` not found.\")\n",
    "\n",
    "# ----- XGBoost if present -----\n",
    "if \"xgb_model\" in globals():\n",
    "    import xgboost as xgb\n",
    "    dval  = xgb.DMatrix(X_val,  label=y_val)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    val_prob_xgb  = xgb_model.predict(dval,  iteration_range=(0, xgb_model.best_iteration + 1))\n",
    "    t_xgb, _      = best_threshold(y_val, val_prob_xgb)\n",
    "\n",
    "    test_prob_xgb = xgb_model.predict(dtest, iteration_range=(0, xgb_model.best_iteration + 1))\n",
    "    m_xgb = compute_metrics(y_test, test_prob_xgb, threshold=t_xgb)\n",
    "    rows.append({\n",
    "        \"model\": \"XGBoost\",\n",
    "        \"val_best_thr\": t_xgb,\n",
    "        **m_xgb\n",
    "    })\n",
    "else:\n",
    "    print(\"↪️ Skipping XGBoost: variable `xgb_model` not found.\")\n",
    "\n",
    "# ----- MLP (torch) if present -----\n",
    "if \"mlp\" in globals():\n",
    "    val_prob_mlp = predict_proba_torch(mlp, val_loader)\n",
    "    t_mlp, _     = best_threshold(y_val, val_prob_mlp)\n",
    "\n",
    "    test_prob_mlp = predict_proba_torch(mlp, test_loader)\n",
    "    m_mlp = compute_metrics(y_test, test_prob_mlp, threshold=t_mlp)\n",
    "    rows.append({\n",
    "        \"model\": \"MLP_Torch\",\n",
    "        \"val_best_thr\": t_mlp,\n",
    "        **m_mlp\n",
    "    })\n",
    "else:\n",
    "    print(\"↪️ Skipping MLP: variable `mlp` not found.\")\n",
    "\n",
    "# ----- RandomForest if present -----\n",
    "if \"rf_model\" in globals():\n",
    "    # Use validation set to find the best threshold for F1 score\n",
    "    val_prob_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "    t_rf, _     = best_threshold(y_val, val_prob_rf)\n",
    "\n",
    "    # Use the test set to get final performance metrics\n",
    "    test_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "    m_rf = compute_metrics(y_test, test_prob_rf, threshold=t_rf)\n",
    "    rows.append({\n",
    "        \"model\": \"RandomForest\",\n",
    "        \"val_best_thr\": t_rf,\n",
    "        **m_rf\n",
    "    })\n",
    "else:\n",
    "    print(\"↪️ Skipping RandomForest: variable `rf_model` not found.\")\n",
    "\n",
    "\n",
    "# ----- Show comparison -----\n",
    "cols = [\"model\", \"val_best_thr\", \"roc_auc\", \"accuracy\", \"f1\", \"precision\", \"recall\"]\n",
    "comparison_df = pd.DataFrame(rows)[cols].sort_values(by=[\"roc_auc\",\"f1\"], ascending=False)\n",
    "display(comparison_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed5670b9-c66d-458a-aea3-1a42424386e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2e4f5826b0407e9995aa110b5ab1ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Tuning Progress:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuning RandomForest with RandomizedSearchCV ---\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "\n",
      "✅ Best parameters for RandomForest: {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': 30}\n",
      "🏆 Best cross-validated F1-score: 0.9244\n",
      "🏃 View run random_tuning_RandomForest at: http://127.0.0.1:5000/#/experiments/1/runs/c1a7f98460984e41930a422b0afe8942\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "\n",
      "--- Tuning XGBoost with RandomizedSearchCV ---\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "\n",
      "✅ Best parameters for XGBoost: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.05}\n",
      "🏆 Best cross-validated F1-score: 0.9247\n",
      "🏃 View run random_tuning_XGBoost at: http://127.0.0.1:5000/#/experiments/1/runs/e01b0795985a45e3b68aff4ac1b9f32b\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "\n",
      "✅ Hyperparameter tuning complete.\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 9 (Final Version) — Stable Tuning with Manual Logging =====\n",
    "import mlflow\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from tqdm.auto import tqdm # Import tqdm for the progress bar\n",
    "\n",
    "# --- 1. Define Models and Parameter Grids ---\n",
    "models_to_tune = {\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "        \"params\": {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model\": xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
    "        \"params\": {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.05, 0.1, 0.2],\n",
    "            'max_depth': [5, 7, 10],\n",
    "            'subsample': [0.7, 0.8, 1.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- 2. Run RandomizedSearchCV for each model ---\n",
    "\n",
    "best_estimators = {}\n",
    "\n",
    "# Wrap the items() with tqdm to show progress for each model being tuned\n",
    "for model_name, config in tqdm(models_to_tune.items(), desc=\"Overall Tuning Progress\"):\n",
    "    with mlflow.start_run(run_name=f\"random_tuning_{model_name}\") as parent_run:\n",
    "        print(f\"\\n--- Tuning {model_name} with RandomizedSearchCV ---\")\n",
    "        \n",
    "        # NOTE: mlflow.sklearn.autolog() has been REMOVED to prevent crashes.\n",
    "\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=config[\"model\"],\n",
    "            param_distributions=config[\"params\"],\n",
    "            n_iter=10,\n",
    "            cv=3,\n",
    "            scoring='f1',\n",
    "            verbose=1,\n",
    "            random_state=42,\n",
    "            n_jobs=1  # Keep at 1 for stability\n",
    "        )\n",
    "        \n",
    "        random_search.fit(X_train, y_train)\n",
    "\n",
    "        print(f\"\\n✅ Best parameters for {model_name}: {random_search.best_params_}\")\n",
    "        print(f\"🏆 Best cross-validated F1-score: {random_search.best_score_:.4f}\")\n",
    "        \n",
    "        # --- Manually log the most important results to the parent run ---\n",
    "        mlflow.log_metric(\"best_cv_f1_score\", random_search.best_score_)\n",
    "        mlflow.log_params(random_search.best_params_)\n",
    "\n",
    "        best_estimators[model_name] = random_search.best_estimator_\n",
    "\n",
    "print(\"\\n✅ Hyperparameter tuning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b99d50e-ec38-4b1c-a0b8-b789589d23cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>val_best_thr</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost_Tuned</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.980489</td>\n",
       "      <td>0.864791</td>\n",
       "      <td>0.888269</td>\n",
       "      <td>0.814917</td>\n",
       "      <td>0.976132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest_Tuned</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.979094</td>\n",
       "      <td>0.856909</td>\n",
       "      <td>0.882810</td>\n",
       "      <td>0.803921</td>\n",
       "      <td>0.978867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  val_best_thr   roc_auc  accuracy        f1  precision  \\\n",
       "1       XGBoost_Tuned           0.5  0.980489  0.864791  0.888269   0.814917   \n",
       "0  RandomForest_Tuned           0.5  0.979094  0.856909  0.882810   0.803921   \n",
       "\n",
       "     recall  \n",
       "1  0.976132  \n",
       "0  0.978867  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 10 — Compare FINAL TUNED models on TEST =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# The best_threshold function remains the same\n",
    "def best_threshold(y_true, y_prob):\n",
    "    \"\"\"Grid-search threshold in [0,1] to maximize F1.\"\"\"\n",
    "    ts = np.linspace(0.05, 0.95, 19)\n",
    "    best_t, best_f1 = 0.5, -1.0\n",
    "    for t in ts:\n",
    "        f1 = compute_metrics(y_true, (y_prob >= t).astype(int))[\"f1\"]\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, t\n",
    "    return float(best_t), float(best_f1)\n",
    "\n",
    "rows = []\n",
    "\n",
    "# --- Use the 'best_estimators' dictionary from the tuning cell (Cell 9) ---\n",
    "\n",
    "# ----- Tuned RandomForest -----\n",
    "if \"RandomForest\" in best_estimators:\n",
    "    tuned_rf = best_estimators[\"RandomForest\"]\n",
    "    \n",
    "    # Use validation set to find the best threshold for F1 score\n",
    "    val_prob_rf = tuned_rf.predict_proba(X_val)[:, 1]\n",
    "    t_rf, _     = best_threshold(y_val, val_prob_rf)\n",
    "\n",
    "    # Use the test set to get final performance metrics\n",
    "    test_prob_rf = tuned_rf.predict_proba(X_test)[:, 1]\n",
    "    m_rf = compute_metrics(y_test, test_prob_rf, threshold=t_rf)\n",
    "    rows.append({\n",
    "        \"model\": \"RandomForest_Tuned\",\n",
    "        \"val_best_thr\": t_rf,\n",
    "        **m_rf\n",
    "    })\n",
    "else:\n",
    "    print(\"↪️ Skipping Tuned RandomForest: model not found in best_estimators.\")\n",
    "\n",
    "# ----- Tuned XGBoost -----\n",
    "if \"XGBoost\" in best_estimators:\n",
    "    tuned_xgb = best_estimators[\"XGBoost\"]\n",
    "\n",
    "    val_prob_xgb  = tuned_xgb.predict_proba(X_val)[:, 1]\n",
    "    t_xgb, _      = best_threshold(y_val, val_prob_xgb)\n",
    "\n",
    "    test_prob_xgb = tuned_xgb.predict_proba(X_test)[:, 1]\n",
    "    m_xgb = compute_metrics(y_test, test_prob_xgb, threshold=t_xgb)\n",
    "    rows.append({\n",
    "        \"model\": \"XGBoost_Tuned\",\n",
    "        \"val_best_thr\": t_xgb,\n",
    "        **m_xgb\n",
    "    })\n",
    "else:\n",
    "    print(\"↪️ Skipping Tuned XGBoost: model not found in best_estimators.\")\n",
    "\n",
    "# ----- Show final comparison -----\n",
    "cols = [\"model\", \"val_best_thr\", \"roc_auc\", \"accuracy\", \"f1\", \"precision\", \"recall\"]\n",
    "if rows:\n",
    "    comparison_df = pd.DataFrame(rows)[cols].sort_values(by=[\"roc_auc\", \"f1\"], ascending=False)\n",
    "    display(comparison_df)\n",
    "else:\n",
    "    print(\"No tuned models were found to evaluate.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c5ecc04-e40b-4aee-b518-5a59c4982db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best XGBoost model saved to: best_xgboost_model.pkl\n",
      "✅ Best Random Forest model saved to: best_randomforest_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 11 — Create Model Artifacts =====\n",
    "import pickle\n",
    "\n",
    "# --- 1. Define artifact filenames ---\n",
    "BEST_XGBOOST_FILENAME = \"best_xgboost_model.pkl\"\n",
    "BEST_RF_FILENAME = \"best_randomforest_model.pkl\"\n",
    "\n",
    "# --- 2. Save the best tuned XGBoost model ---\n",
    "# We select the tuned XGBoost model from the 'best_estimators' dictionary created during hyperparameter tuning.\n",
    "if \"XGBoost\" in best_estimators:\n",
    "    best_xgb_model = best_estimators[\"XGBoost\"]\n",
    "    with open(BEST_XGBOOST_FILENAME, \"wb\") as f:\n",
    "        pickle.dump(best_xgb_model, f)\n",
    "    print(f\"✅ Best XGBoost model saved to: {BEST_XGBOOST_FILENAME}\")\n",
    "else:\n",
    "    print(\"↪️ Could not save best_xgboost_model.pkl: Tuned XGBoost model not found.\")\n",
    "\n",
    "# --- 3. Save the best tuned Random Forest model ---\n",
    "# We select the tuned Random Forest model from the 'best_estimators' dictionary.\n",
    "if \"RandomForest\" in best_estimators:\n",
    "    best_rf_model = best_estimators[\"RandomForest\"]\n",
    "    with open(BEST_RF_FILENAME, \"wb\") as f:\n",
    "        pickle.dump(best_rf_model, f)\n",
    "    print(f\"✅ Best Random Forest model saved to: {BEST_RF_FILENAME}\")\n",
    "else:\n",
    "    print(\"↪️ Could not save best_randomforest_model.pkl: Tuned Random Forest model not found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d35c94d-4912-4231-81ba-d97eacdd68a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features the model was trained on: 34\n"
     ]
    }
   ],
   "source": [
    "# Find the number of features\n",
    "print(f\"Number of features the model was trained on: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce2920-6fcf-4521-ae3f-3de374cd19ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops_env)",
   "language": "python",
   "name": "mlops_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
