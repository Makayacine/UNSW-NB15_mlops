{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13702a5a-3acf-444d-8041-a9c0fbc8abf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/mlflow/artifacts/1', creation_time=1756847680158, experiment_id='1', last_update_time=1756847680158, lifecycle_stage='active', name='unsw-nb15', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGB-only notebook — CELL 1: setup (use existing MLflow server)\n",
    "from pathlib import Path\n",
    "import os, mlflow\n",
    "\n",
    "# Quiet GitPython warnings inside container\n",
    "os.environ.setdefault(\"GIT_PYTHON_REFRESH\", \"quiet\")\n",
    "\n",
    "BASE = Path(\"/tf/notebooks/ids_unsw\")\n",
    "DATA_DIR  = BASE / \"data\"\n",
    "MODEL_DIR = BASE / \"models\"\n",
    "\n",
    "# Change tracking URI so this container can see the MLflow server\n",
    "mlflow.set_tracking_uri(\"http://host.docker.internal:5000\")  # or \"http://mlflow:5000\"\n",
    "EXPERIMENT_NAME = \"unsw-nb15\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d5bb4c-f9ac-4985-b7e4-d00b603e978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 34 feature names from: /tf/notebooks/ids_unsw/models/feature_names.json\n",
      "Loaded model: XGBClassifier\n",
      "n_estimators: 200 | max_depth: 10\n"
     ]
    }
   ],
   "source": [
    "# XGB-only notebook — CELL 2: load features + model\n",
    "import json, pickle\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR = Path(\"/tf/notebooks/ids_unsw/models\")\n",
    "FEATURES_PATH = MODEL_DIR / \"feature_names.json\"\n",
    "XGB_PKL_PATH  = MODEL_DIR / \"best_xgboost_model.pkl\"  # from your 04 notebook\n",
    "\n",
    "# 1) load feature list\n",
    "with open(FEATURES_PATH, \"r\") as f:\n",
    "    FEATURES = json.load(f)\n",
    "print(f\"Loaded {len(FEATURES)} feature names from:\", FEATURES_PATH)\n",
    "\n",
    "# 2) load the trained XGB model\n",
    "with open(XGB_PKL_PATH, \"rb\") as f:\n",
    "    xgb = pickle.load(f)\n",
    "print(\"Loaded model:\", type(xgb).__name__)\n",
    "try:\n",
    "    print(\"n_estimators:\", getattr(xgb, \"n_estimators\", \"n/a\"),\n",
    "          \"| max_depth:\", getattr(xgb, \"max_depth\", \"n/a\"))\n",
    "except Exception as e:\n",
    "    print(\"Model introspection note:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "425325b6-318c-4089-92f4-b784eef8515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test shape: (82332, 36)\n",
      "Loaded scaler object.\n",
      "X_test shape after scaling: (82332, 34) (n_features expected=34)\n",
      "y_test base rate (mean of 1's): 0.550600\n"
     ]
    }
   ],
   "source": [
    "# ===== Corrected Cell — Load Test Data and Apply Scaling (no feature-name warning) =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# --- Define Paths ---\n",
    "DATA_DIR = Path(\"/tf/notebooks/ids_unsw/data\")\n",
    "MODEL_DIR = Path(\"/tf/notebooks/ids_unsw/models\")\n",
    "TEST_PARQUET = DATA_DIR / \"UNSW_NB15_test_clean.parquet\"\n",
    "SCALER_PATH = MODEL_DIR / \"scaler.pkl\"\n",
    "\n",
    "# --- 1) Load Test Data ---\n",
    "df_test = pd.read_parquet(TEST_PARQUET)\n",
    "print(\"Loaded test shape:\", df_test.shape)\n",
    "\n",
    "# --- 2) Find Label Column ---\n",
    "label_col_candidates = [\"label\", \"y\", \"is_attack\", \"target\"]\n",
    "label_col = next((c for c in df_test.columns if c in label_col_candidates), None)\n",
    "assert label_col is not None, f\"Could not find label column among {label_col_candidates}. Columns: {list(df_test.columns)[:10]}...\"\n",
    "\n",
    "# --- 3) Load the Scaler ---\n",
    "assert SCALER_PATH.exists(), f\"Scaler not found at {SCALER_PATH}. Please ensure it has been saved from the training notebook.\"\n",
    "with open(SCALER_PATH, \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "print(\"Loaded scaler object.\")\n",
    "\n",
    "# --- 4) Prepare Test Features and Labels ---\n",
    "# Convert to NumPy before scaling to match how the scaler was fitted (no feature names)\n",
    "X_test_np = df_test.loc[:, FEATURES].to_numpy(dtype=np.float32, copy=False)\n",
    "X_test = scaler.transform(X_test_np).astype(np.float32, copy=False)\n",
    "\n",
    "y_test = df_test[label_col].astype(int)\n",
    "\n",
    "print(f\"X_test shape after scaling: {X_test.shape} (n_features expected={len(FEATURES)})\")\n",
    "print(f\"y_test base rate (mean of 1's): {y_test.mean():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4fe74b2-e78b-43a9-885a-df2720ce2fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proba shape: (82332,)\n",
      "Proba quantiles [min,1%,5%,50%,95%,99%,max]: [6.70561712468043e-05, 8.351331780431792e-05, 0.00011126959725515917, 0.8789591789245605, 0.9989271759986877, 0.9992443335056305, 0.999703586101532]\n",
      "First 10 probs: [0.9147 0.8796 0.7478 0.5187 0.5628 0.6867 0.5735 0.8126 0.0012 0.0012]\n"
     ]
    }
   ],
   "source": [
    "# ===== Corrected Cell 4 — Predict Probabilities =====\n",
    "import numpy as np\n",
    "\n",
    "# Corrected: X_test is already a NumPy array, so we pass it directly to the model.\n",
    "proba = xgb.predict_proba(X_test)[:, 1]  # P(class=1)\n",
    "\n",
    "print(\"Proba shape:\", proba.shape)\n",
    "\n",
    "q = np.quantile(proba, [0, 0.01, 0.05, 0.50, 0.95, 0.99, 1.0])\n",
    "print(\"Proba quantiles [min,1%,5%,50%,95%,99%,max]:\", [float(v) for v in q])\n",
    "print(\"First 10 probs:\", np.round(proba[:10], 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04eabbfb-ec3d-4237-bc63-d4dbe7abd243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top of sweep:\n",
      "    threshold  precision    recall        f1       FPR     TP     FP     TN  \\\n",
      "0   0.500000   0.814917  0.976132  0.888269  0.271622  44250  10050  26950   \n",
      "1   0.532143   0.828455  0.970948  0.894060  0.246324  44015   9114  27886   \n",
      "2   0.564286   0.843241  0.965675  0.900315  0.219946  43776   8138  28862   \n",
      "3   0.596429   0.856453  0.959080  0.904866  0.196946  43477   7287  29713   \n",
      "4   0.628571   0.869521  0.952594  0.909164  0.175135  43183   6480  30520   \n",
      "\n",
      "     FN  \n",
      "0  1082  \n",
      "1  1317  \n",
      "2  1556  \n",
      "3  1855  \n",
      "4  2149   \n",
      "\n",
      "Bottom of sweep:\n",
      "     threshold  precision    recall        f1  FPR     TP  FP     TN     FN\n",
      "22   0.998112        1.0  0.341789  0.509453  0.0  15494   0  37000  29838\n",
      "23   0.998438        1.0  0.269765  0.424906  0.0  12229   0  37000  33103\n",
      "24   0.998703        1.0  0.179983  0.305061  0.0   8159   0  37000  37173\n",
      "25   0.998898        1.0  0.099466  0.180935  0.0   4509   0  37000  40823\n",
      "26   0.999244        1.0  0.018177  0.035705  0.0    824   0  37000  44508\n"
     ]
    }
   ],
   "source": [
    "# XGB-only notebook — CELL 5: sweep thresholds\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "y_true = y_test.values if hasattr(y_test, \"values\") else y_test\n",
    "\n",
    "def sweep_thresholds(y, proba):\n",
    "    # cover the range well: fixed grid + data-driven quantiles\n",
    "    grid = np.linspace(0.50, 0.95, 15)\n",
    "    qs   = np.quantile(proba, np.linspace(0.50, 0.99, 12))\n",
    "    thresholds = np.unique(np.clip(np.concatenate([grid, qs]), 0, 1))\n",
    "\n",
    "    rows = []\n",
    "    P = (y == 1).sum()\n",
    "    N = (y == 0).sum()\n",
    "    for t in thresholds:\n",
    "        y_hat = (proba >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y, y_hat, labels=[0,1]).ravel()\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y, y_hat, average=\"binary\", zero_division=0\n",
    "        )\n",
    "        fpr = fp / (fp + tn) if (fp + tn) else 0.0\n",
    "        rows.append(dict(threshold=float(t),\n",
    "                         precision=float(prec), recall=float(rec), f1=float(f1),\n",
    "                         FPR=float(fpr), TP=int(tp), FP=int(fp), TN=int(tn), FN=int(fn)))\n",
    "    return pd.DataFrame(rows).sort_values(\"threshold\").reset_index(drop=True)\n",
    "\n",
    "sweep_xgb = sweep_thresholds(y_true, proba)\n",
    "\n",
    "print(\"Top of sweep:\\n\", sweep_xgb.head(), \"\\n\")\n",
    "print(\"Bottom of sweep:\\n\", sweep_xgb.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72481d07-d34d-4c0e-b123-04af4457e5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold = 0.6286\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.869521</td>\n",
       "      <td>0.952594</td>\n",
       "      <td>0.909164</td>\n",
       "      <td>0.175135</td>\n",
       "      <td>43183.0</td>\n",
       "      <td>6480.0</td>\n",
       "      <td>30520.0</td>\n",
       "      <td>2149.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   threshold  precision    recall        f1       FPR       TP      FP  \\\n",
       "4   0.628571   0.869521  0.952594  0.909164  0.175135  43183.0  6480.0   \n",
       "\n",
       "        TN      FN  \n",
       "4  30520.0  2149.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# XGB-only notebook — CELL 6: choose operating threshold\n",
    "import numpy as np\n",
    "\n",
    "# y_true and proba already defined; sweep_xgb already computed in the last cell\n",
    "eligible = sweep_xgb[sweep_xgb[\"recall\"] >= 0.95].copy()\n",
    "\n",
    "if eligible.empty:\n",
    "    print(\"No threshold reaches recall ≥ 0.95 — falling back to the row with highest recall.\")\n",
    "    chosen = sweep_xgb.sort_values([\"recall\",\"threshold\"], ascending=[False, True]).iloc[0]\n",
    "else:\n",
    "    # minimize FPR; tie-breaker: maximize precision; then prefer the higher threshold\n",
    "    chosen = (\n",
    "        eligible.sort_values([\"FPR\", \"precision\", \"threshold\"],\n",
    "                             ascending=[True, False, True])\n",
    "                .iloc[0]\n",
    "    )\n",
    "\n",
    "thr = float(chosen[\"threshold\"])\n",
    "print(f\"Chosen threshold = {thr:.4f}\")\n",
    "display(chosen.to_frame().T)  # shows the full row nicely if you're in a notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf6eb2ff-8c05-4b94-a0e5-16c27b4fbb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proba_xgb shape: (82332,) | thr: 0.757143\n"
     ]
    }
   ],
   "source": [
    "# CELL — recompute probabilities & set the chosen threshold\n",
    "import numpy as np\n",
    "\n",
    "# assumes `xgb` (XGBClassifier) and `X_test` are already loaded\n",
    "proba_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "thr = 0.757143  # your chosen threshold from the sweep\n",
    "\n",
    "print(\"proba_xgb shape:\", proba_xgb.shape, \"| thr:\", thr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b07f9b7-7b3a-4b8c-b1a8-8b98befaffe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion: TN=34258, FP=2742, FN=3560, TP=41772\n",
      "{'precision': 0.9384014018061734, 'recall': 0.9214682784787788, 'f1': 0.9298577566057477, 'roc_auc': 0.9804886709704498, 'FPR': 0.07410810810810811, 'threshold': 0.757143}\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9059    0.9259    0.9158     37000\n",
      "           1     0.9384    0.9215    0.9299     45332\n",
      "\n",
      "    accuracy                         0.9235     82332\n",
      "   macro avg     0.9221    0.9237    0.9228     82332\n",
      "weighted avg     0.9238    0.9235    0.9235     82332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 7 — finalize eval at chosen threshold (XGB-only)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_recall_fscore_support, roc_auc_score\n",
    ")\n",
    "\n",
    "# proba_xgb: array of P(class=1), y_test: true labels (0/1), thr: chosen threshold\n",
    "y_true = np.asarray(y_test)\n",
    "y_pred = (proba_xgb >= thr).astype(int)\n",
    "\n",
    "# Confusion matrix in TN, FP, FN, TP order\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "fpr = fp / (fp + tn) if (fp + tn) else 0.0\n",
    "\n",
    "# Thresholded metrics\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=\"binary\", zero_division=0\n",
    ")\n",
    "\n",
    "# Threshold-free metric for reference\n",
    "roc_auc = roc_auc_score(y_true, proba_xgb)\n",
    "\n",
    "print(f\"Confusion: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
    "print({\n",
    "    \"precision\": float(prec),\n",
    "    \"recall\": float(rec),\n",
    "    \"f1\": float(f1),\n",
    "    \"roc_auc\": float(roc_auc),\n",
    "    \"FPR\": float(fpr),\n",
    "    \"threshold\": float(thr),\n",
    "})\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f98600-dc83-4a13-8d03-1bf67f89fb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved threshold + metrics to /tf/notebooks/ids_unsw/models/metadata.json\n",
      "🏃 View run xgb_threshold@0.7571 at: http://host.docker.internal:5000/#/experiments/1/runs/1305f0b763514b2cba5ea26da06a5944\n",
      "🧪 View experiment at: http://host.docker.internal:5000/#/experiments/1\n",
      "✅ Logged to MLflow.\n"
     ]
    }
   ],
   "source": [
    "# CELL 8 — persist threshold & metrics, and log to MLflow (server)\n",
    "import os, json, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_recall_fscore_support\n",
    "import mlflow\n",
    "\n",
    "# ensure MLflow points at the running server + experiment name used earlier\n",
    "os.environ.setdefault(\"GIT_PYTHON_REFRESH\", \"quiet\")\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"http://host.docker.internal:5000\"))\n",
    "mlflow.set_experiment(\"unsw-nb15\")\n",
    "\n",
    "# assumes: MODEL_DIR, y_test, proba_xgb, X_test already exist in memory\n",
    "thr = 0.757143  # chosen threshold\n",
    "\n",
    "y_true = np.asarray(y_test)\n",
    "y_pred = (proba_xgb >= thr).astype(int)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "auc  = roc_auc_score(y_true, proba_xgb)\n",
    "fpr  = fp / (fp + tn)\n",
    "\n",
    "# ---- save/update local metadata (models/metadata.json)\n",
    "meta_path = MODEL_DIR / \"metadata.json\"\n",
    "meta = {}\n",
    "if meta_path.exists():\n",
    "    meta = json.loads(meta_path.read_text())\n",
    "\n",
    "meta.update({\n",
    "    \"champion\": \"xgboost\",\n",
    "    \"threshold\": float(thr),\n",
    "    \"n_features\": int(X_test.shape[1]),\n",
    "    \"metrics_at_threshold\": {\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"roc_auc\": float(auc),\n",
    "        \"FPR\": float(fpr),\n",
    "        \"TP\": int(tp), \"FP\": int(fp), \"TN\": int(tn), \"FN\": int(fn),\n",
    "    },\n",
    "})\n",
    "meta_path.write_text(json.dumps(meta, indent=2))\n",
    "print(f\"✅ Saved threshold + metrics to {meta_path}\")\n",
    "\n",
    "# ---- log to MLflow (artifacts under xgb/)\n",
    "with mlflow.start_run(run_name=f\"xgb_threshold@{thr:.4f}\"):\n",
    "    mlflow.log_params({\n",
    "        \"model\": \"xgboost\",\n",
    "        \"threshold\": float(thr),\n",
    "        \"n_features\": int(X_test.shape[1]),\n",
    "    })\n",
    "    mlflow.log_metrics({\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"roc_auc\": float(auc),\n",
    "        \"FPR\": float(fpr),\n",
    "        \"TP\": int(tp), \"FP\": int(fp), \"TN\": int(tn), \"FN\": int(fn),\n",
    "    })\n",
    "\n",
    "    # artifacts\n",
    "    if (MODEL_DIR / \"xgb.onnx\").exists():\n",
    "        mlflow.log_artifact(str(MODEL_DIR / \"xgb.onnx\"), artifact_path=\"xgb\")\n",
    "    if (MODEL_DIR / \"feature_names.json\").exists():\n",
    "        mlflow.log_artifact(str(MODEL_DIR / \"feature_names.json\"), artifact_path=\"xgb\")\n",
    "    # push the same metadata.json that we saved locally\n",
    "    mlflow.log_artifact(str(meta_path), artifact_path=\"xgb\")\n",
    "\n",
    "print(\"✅ Logged to MLflow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c095f6f0-9cae-480d-bed8-12376511ffeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'unsw_xgb_ids_onnx_20250903-231006'.\n",
      "2025/09/03 23:10:10 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: unsw_xgb_ids_onnx_20250903-231006, version 1\n",
      "Created version '1' of model 'unsw_xgb_ids_onnx_20250903-231006'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run register_xgb_onnx at: http://host.docker.internal:5000/#/experiments/1/runs/f2a627e144e84f93a15edb6db76161aa\n",
      "🧪 View experiment at: http://host.docker.internal:5000/#/experiments/1\n",
      "✅ Registered as `unsw_xgb_ids_onnx_20250903-231006`. Open: http://host.docker.internal:5000/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# CELL 9 — register ONNX in MLflow Model Registry (auto-unique name)\n",
    "import os, onnx, mlflow, mlflow.onnx\n",
    "import numpy as np, time, json\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"/tf/notebooks/ids_unsw\")\n",
    "MODEL_DIR = BASE / \"models\"\n",
    "\n",
    "os.environ.setdefault(\"GIT_PYTHON_REFRESH\", \"quiet\")\n",
    "mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\", \"http://host.docker.internal:5000\"))\n",
    "mlflow.set_experiment(\"unsw-nb15\")\n",
    "\n",
    "# --- load ONNX\n",
    "onnx_path = MODEL_DIR / \"xgb.onnx\"\n",
    "assert onnx_path.exists(), f\"Missing {onnx_path}\"\n",
    "onnx_model = onnx.load(str(onnx_path))\n",
    "\n",
    "# --- infer feature count for input_example\n",
    "feat_file = MODEL_DIR / \"feature_names.json\"\n",
    "if feat_file.exists():\n",
    "    features = json.loads(feat_file.read_text())\n",
    "    n_features = len(features)\n",
    "else:\n",
    "    n_features = 34\n",
    "\n",
    "input_example = np.zeros((1, n_features), dtype=np.float32)\n",
    "\n",
    "# --- decide model registry name\n",
    "base_name = \"unsw_xgb_ids_onnx\"\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "existing = client.search_registered_models(filter_string=f\"name='{base_name}'\")\n",
    "if existing:  \n",
    "    # model already exists → create unique one\n",
    "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    model_name = f\"{base_name}_{ts}\"\n",
    "else:\n",
    "    model_name = base_name\n",
    "\n",
    "# --- log & register ONNX\n",
    "with mlflow.start_run(run_name=\"register_xgb_onnx\"):\n",
    "    mlflow.onnx.log_model(\n",
    "        onnx_model,\n",
    "        name=\"xgb\",\n",
    "        registered_model_name=model_name,\n",
    "        input_example=input_example\n",
    "    )\n",
    "    for name in (\"feature_names.json\", \"metadata.json\"):\n",
    "        p = MODEL_DIR / name\n",
    "        if p.exists():\n",
    "            mlflow.log_artifact(str(p), artifact_path=\"xgb\")\n",
    "\n",
    "info = mlflow.get_experiment_by_name(\"unsw-nb15\")\n",
    "print(f\"✅ Registered as `{model_name}`. Open:\", f\"http://host.docker.internal:5000/#/experiments/{info.experiment_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17d1be2a-bb98-470c-a5c6-abf5ba3f9d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Bundle contents: ['feature_names.json', 'metadata.json', 'xgb.onnx']\n"
     ]
    }
   ],
   "source": [
    "# CELL 10 — assemble a local serving bundle\n",
    "from pathlib import Path\n",
    "import shutil, json\n",
    "\n",
    "BASE = Path(\"/tf/notebooks/ids_unsw\")\n",
    "MODEL_DIR = BASE / \"models\"\n",
    "BUNDLE = MODEL_DIR / \"bundle_xgb\"\n",
    "BUNDLE.mkdir(exist_ok=True)\n",
    "\n",
    "for name in (\"xgb.onnx\", \"feature_names.json\", \"metadata.json\"):\n",
    "    src = MODEL_DIR / name\n",
    "    if src.exists():\n",
    "        shutil.copy2(src, BUNDLE / name)\n",
    "\n",
    "print(\"📦 Bundle contents:\", [p.name for p in sorted(BUNDLE.iterdir())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf56dfe6-4567-4c5b-8b36-8f3ec795e530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs[:5] = [0.9147 0.8796 0.7478 0.5187 0.5628]\n",
      "preds[:5] = [1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# ===== Corrected Cell 11 — Minimal ONNX Prediction Check =====\n",
    "import json\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "import pandas as pd # <-- Make sure pandas is imported\n",
    "\n",
    "BASE = Path(\"/tf/notebooks/ids_unsw\")\n",
    "BUNDLE = BASE / \"models\" / \"bundle_xgb\"\n",
    "\n",
    "features = json.loads((BUNDLE/\"feature_names.json\").read_text())\n",
    "meta = json.loads((BUNDLE/\"metadata.json\").read_text())\n",
    "thr = float(meta[\"threshold\"])\n",
    "\n",
    "sess = ort.InferenceSession(str(BUNDLE/\"xgb.onnx\"), providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def score_df(df):\n",
    "    X = df[features].to_numpy(np.float32)\n",
    "    # The ONNX model for XGBoost often returns probabilities in the second output tensor\n",
    "    # The [1] selects that tensor, and [:,1] selects the probability for the positive class (1)\n",
    "    probs = sess.run(None, {\"input\": X})[1][:,1]\n",
    "    preds = (probs >= thr).astype(np.int32)\n",
    "    return probs, preds\n",
    "\n",
    "# --- CORRECTED CODE ---\n",
    "# Create a small test DataFrame from the first 10 rows of our NumPy arrays\n",
    "# 'features' is the list of column names loaded from the JSON file.\n",
    "quick_test_df = pd.DataFrame(X_test[:10], columns=features)\n",
    "\n",
    "# Since y_test is a separate array, we add it back if needed for context,\n",
    "# but score_df only needs the feature columns.\n",
    "probs, preds = score_df(quick_test_df)\n",
    "# --- END CORRECTED CODE ---\n",
    "\n",
    "print(\"probs[:5] =\", np.round(probs[:5], 4))\n",
    "print(\"preds[:5] =\", preds[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe4e4ee1-7cbd-4f9e-8a5b-911129cff626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB GPU (DMatrix): 87.4 ms — 942,528 samples/sec\n",
      "XGB GPU (CuPy + inplace_predict): 31.3 ms — 2,628,343 samples/sec\n"
     ]
    }
   ],
   "source": [
    "# Section 12 — Option A: XGBoost GPU Predictor (no conversion)\n",
    "# Runs your trained model on the GPU. Avoids timing the prediction cache.\n",
    "\n",
    "import time, numpy as np\n",
    "from xgboost import DMatrix\n",
    "\n",
    "# Use float32 for best throughput\n",
    "X_np = np.asarray(X_test, dtype=np.float32)\n",
    "\n",
    "# Get Booster and select GPU (no deprecated 'predictor' param)\n",
    "booster = xgb.get_booster()\n",
    "booster.set_param({'device': 'cuda'})\n",
    "\n",
    "# --------- Path A: GPU via DMatrix (works without CuPy) ----------\n",
    "# Warm-up on a tiny DMatrix to prime kernels (doesn't cache the big one)\n",
    "_ = booster.predict(DMatrix(X_np[:2048]))\n",
    "\n",
    "# Time **fresh** DMatrix to avoid cached predictions\n",
    "t0 = time.perf_counter()\n",
    "proba_gpu_dmatrix = booster.predict(DMatrix(X_np))    # P(class=1) for binary:logistic\n",
    "t1 = time.perf_counter()\n",
    "dt_dmat = t1 - t0\n",
    "print(f\"XGB GPU (DMatrix): {dt_dmat*1000:.1f} ms — {X_np.shape[0]/dt_dmat:,.0f} samples/sec\")\n",
    "\n",
    "# --------- Path B: GPU via inplace_predict + CuPy (fastest if available) ----------\n",
    "proba_gpu_inplace = None\n",
    "try:\n",
    "    import cupy as cp\n",
    "    X_gpu = cp.asarray(X_np)\n",
    "\n",
    "    # warm-up\n",
    "    _ = booster.inplace_predict(X_gpu[:2048])\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    proba_gpu_inplace = booster.inplace_predict(X_gpu)  # CuPy vector, P(class=1)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    dt_inplace = t1 - t0\n",
    "    print(f\"XGB GPU (CuPy + inplace_predict): {dt_inplace*1000:.1f} ms — {X_np.shape[0]/dt_inplace:,.0f} samples/sec\")\n",
    "except Exception:\n",
    "    print(\"CuPy not available; skipped inplace_predict path (install: pip install cupy-cuda12x).\")\n",
    "\n",
    "# Choose probabilities to carry forward (numpy array)\n",
    "if proba_gpu_inplace is not None:\n",
    "    try:\n",
    "        proba_xgb_gpu = cp.asnumpy(proba_gpu_inplace)\n",
    "    except Exception:\n",
    "        proba_xgb_gpu = proba_gpu_inplace  # already numpy in some builds\n",
    "else:\n",
    "    proba_xgb_gpu = proba_gpu_dmatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c4b7819-f332-455a-a26b-d40054438732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB (CPU, inplace_predict): 48.9 ms — 1,684,618 samples/sec\n",
      "XGB (GPU, DMatrix): 73.6 ms — 1,118,184 samples/sec\n",
      "XGB (GPU, CuPy + inplace_predict): 32.0 ms — 2,574,538 samples/sec\n",
      "ONNXRuntime (CPU): 93.3 ms — 881,993 samples/sec\n"
     ]
    }
   ],
   "source": [
    "# Section 13 — Benchmarking inference (fixed: use Booster.inplace_predict)\n",
    "import time, numpy as np, onnxruntime as ort\n",
    "from xgboost import DMatrix\n",
    "\n",
    "X_np = np.asarray(X_test, dtype=np.float32)\n",
    "n = X_np.shape[0]\n",
    "booster = xgb.get_booster()\n",
    "\n",
    "# --- XGB CPU (Booster.inplace_predict on NumPy)\n",
    "booster.set_param({'device': 'cpu'})\n",
    "t0 = time.perf_counter()\n",
    "try:\n",
    "    proba_cpu_inplace = booster.inplace_predict(X_np)   # P(class=1)\n",
    "except AttributeError:\n",
    "    # very old xgboost: fallback to sklearn API\n",
    "    proba_cpu_inplace = xgb.predict_proba(X_np)[:, 1]\n",
    "t1 = time.perf_counter()\n",
    "dt_cpu = t1 - t0\n",
    "print(f\"XGB (CPU, inplace_predict): {dt_cpu*1000:.1f} ms — {n/dt_cpu:,.0f} samples/sec\")\n",
    "\n",
    "# --- XGB GPU (fresh DMatrix to avoid cache)\n",
    "booster.set_param({'device': 'cuda'})\n",
    "_ = booster.predict(DMatrix(X_np[:2048]))  # warm-up\n",
    "t0 = time.perf_counter()\n",
    "proba_gpu_dmat = booster.predict(DMatrix(X_np))\n",
    "t1 = time.perf_counter()\n",
    "dt_gpu_dmat = t1 - t0\n",
    "print(f\"XGB (GPU, DMatrix): {dt_gpu_dmat*1000:.1f} ms — {n/dt_gpu_dmat:,.0f} samples/sec\")\n",
    "\n",
    "# --- XGB GPU (CuPy + inplace_predict) if available\n",
    "try:\n",
    "    import cupy as cp\n",
    "    X_gpu = cp.asarray(X_np)\n",
    "    _ = booster.inplace_predict(X_gpu[:2048]); cp.cuda.Stream.null.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    proba_gpu_inplace = booster.inplace_predict(X_gpu)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    t1 = time.perf_counter()\n",
    "    dt_gpu_inplace = t1 - t0\n",
    "    print(f\"XGB (GPU, CuPy + inplace_predict): {dt_gpu_inplace*1000:.1f} ms — {n/dt_gpu_inplace:,.0f} samples/sec\")\n",
    "except Exception as e:\n",
    "    proba_gpu_inplace = None\n",
    "    print(f\"XGB (GPU, CuPy) skipped — {e}\")\n",
    "\n",
    "# --- ONNXRuntime (CPU)\n",
    "sess = ort.InferenceSession(str(MODEL_DIR / \"xgb.onnx\"), providers=[\"CPUExecutionProvider\"])\n",
    "inp_name = sess.get_inputs()[0].name\n",
    "out_name = sess.get_outputs()[1].name  # 'probabilities' (Nx2)\n",
    "_ = sess.run([out_name], {inp_name: X_np[:64]})  # warm-up\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "probs2 = sess.run([out_name], {inp_name: X_np})[0]  # shape (n,2)\n",
    "t1 = time.perf_counter()\n",
    "dt_onnx = t1 - t0\n",
    "proba_onnx = probs2[:, 1].astype(np.float32)\n",
    "print(f\"ONNXRuntime (CPU): {dt_onnx*1000:.1f} ms — {n/dt_onnx:,.0f} samples/sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750374e-89ac-4431-8cb2-919a6439f7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
