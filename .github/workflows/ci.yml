name: CI
on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]
  workflow_dispatch:
    inputs:
      image_tag:
        description: 'Optional extra image tag (e.g., v0.1.0)'
        required: false
        default: ''
concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true
env:
  IMAGE_NAME: ${{ secrets.DOCKERHUB_USERNAME }}/ids-unsw-api

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true
      - uses: actions/setup-python@v5
        with: { python-version: "3.11" }
      - run: python -m pip install --upgrade pip
      - run: pip install -r requirements.txt
      - name: Run tests
        run: pytest -q

# --- new job: DATA -----------------------------------------------------------
  data:
    runs-on: ubuntu-latest
    needs: [test]         # optional: run after unit/bundle tests
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - run: pip install -r requirements.txt
      - name: Create tiny raw dataset
        run: |
          python - <<'PY'
          import pandas as pd, numpy as np, pathlib
          out = pathlib.Path("artifacts/raw"); out.mkdir(parents=True, exist_ok=True)
          df = pd.DataFrame({
              "dur": np.linspace(0, 1, 16),
              "sbytes": np.arange(16),
              "label": (np.arange(16) % 2).astype(int),
          })
          df.to_csv(out/"raw.csv", index=False)
          print("Wrote", out/"raw.csv")
          PY
      - uses: actions/upload-artifact@v4
        with:
          name: raw
          path: artifacts/raw
# --- new job: FEATURES -------------------------------------------------------
  features:
    runs-on: ubuntu-latest
    needs: [data]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: raw
          path: work/raw
      - name: Build features
        run: |
          python - <<'PY'
          import pandas as pd, json, pathlib
          raw = pathlib.Path('work/raw/raw.csv')
          df  = pd.read_csv(raw)
          X   = pd.DataFrame({
              'x1': df['dur'],
              'x2': (df['sbytes'] / max(df['sbytes'].max(), 1)),
          })
          y   = df['label']
          out = pathlib.Path('artifacts/features'); out.mkdir(parents=True, exist_ok=True)
          X.to_csv(out/'X.csv', index=False)
          y.to_csv(out/'y.csv', index=False)
          (out/'feature_names.json').write_text(json.dumps({'features': list(X.columns)}))
          PY
      - uses: actions/upload-artifact@v4
        with:
          name: features
          path: artifacts/features
# --- new job: TRAIN ----------------------------------------------------------
  train:
    runs-on: ubuntu-latest
    needs: [features]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - run: pip install -r requirements.txt
      - uses: actions/download-artifact@v4
        with:
          name: features
          path: work/features
      - name: Train & log with MLflow
        run: |
          python - <<'PY'
          import pandas as pd, numpy as np, pathlib, json, mlflow, os, shutil
          X = pd.read_csv('work/features/X.csv').values
          y = pd.read_csv('work/features/y.csv').values.ravel()
          # tiny closed-form "model" using least squares
          X1 = np.c_[np.ones(len(X)), X]
          w, *_ = np.linalg.lstsq(X1, y, rcond=None)
          pred = X1 @ w
          rmse = float(np.sqrt(((pred - y)**2).mean()))
          mlflow.set_tracking_uri('file://' + str(pathlib.Path.cwd()/'mlruns'))
          mlflow.set_experiment('ci-demo')
          with mlflow.start_run(run_name='train-ci'):
              mlflow.log_metric('rmse', rmse)
              np.savez('model.npz', w=w)
              mlflow.log_artifact('model.npz', artifact_path='model')
              pathlib.Path('model').mkdir(exist_ok=True)
              (pathlib.Path('model')/'metrics.json').write_text(json.dumps({'rmse': rmse}))
          out = pathlib.Path('artifacts/train'); out.mkdir(parents=True, exist_ok=True)
          shutil.copytree('mlruns', out/'mlruns', dirs_exist_ok=True)
          print('RMSE:', rmse)
          PY
      - uses: actions/upload-artifact@v4
        with:
          name: mlruns
          path: artifacts/train/mlruns

  docker:
    runs-on: ubuntu-latest
    needs: [test, train]
    steps:
      - name: Checkout (with LFS for ONNX bundle)
        uses: actions/checkout@v4
        with:
          lfs: true
      - uses: actions/setup-python@v5
        with: { python-version: '3.11' }
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      # Build a local tag we can run for tests
      - name: Build image (local test tag)
        run: |
          docker build -f Dockerfile.api -t ids-unsw-api:local .
      - name: Run container
        run: |
          docker rm -f ids-unsw-api 2>/dev/null || true
          docker run -d --name ids-unsw-api -p 8000:8000 \
            -e IDS_API_TOKEN=${{ secrets.IDS_API_TOKEN }} \
            ids-unsw-api:local
      - name: Wait for API (max 60s)
        run: |
          for i in {1..30}; do
            if curl -fsS -H "Authorization: Bearer ${{ secrets.IDS_API_TOKEN }}" \
                 http://localhost:8000/health | grep -q '"status"[[:space:]]*:[[:space:]]*"ok"'; then
              echo "API is up"; exit 0
            fi
            echo "waiting for API... ($i)"; sleep 2
          done
          echo "::error::API never became ready. Logs:"
          docker ps -a
          docker logs ids-unsw-api || true
          exit 1
      - name: Exercise /features
        run: |
          curl -fsS -H "Authorization: Bearer ${{ secrets.IDS_API_TOKEN }}" \
               http://localhost:8000/features | head
      - name: Exercise /predict (zeros payload)
        run: |
          PAYLOAD=$(curl -fsS -H "Authorization: Bearer ${{ secrets.IDS_API_TOKEN }}" \
                          http://localhost:8000/features | python3 - <<'PY'
          import sys,json
          feats=json.load(sys.stdin)["features"]
          print(json.dumps({"instances":[{k:0 for k in feats}]}))
          PY
          )
          echo "$PAYLOAD" | curl -fsS -X POST http://localhost:8000/predict \
            -H 'Content-Type: application/json' \
            -H "Authorization: Bearer ${{ secrets.IDS_API_TOKEN }}" \
            -d @- | head
      - name: Cleanup
        if: always()
        run: docker rm -f ids-unsw-api

      - name: Compute tags/labels
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.IMAGE_NAME }}
          tags: |
            # always
            type=raw,value=sha-${{ github.sha }}
            # main branch gets latest
            type=raw,value=latest,enable=${{ github.ref == 'refs/heads/main' }}
            # PRs get pr-<number>
            type=ref,event=pr
            # manual runs can add an extra tag
            type=raw,value=${{ github.event.inputs.image_tag }},enable=${{ github.event_name == 'workflow_dispatch' && github.event.inputs.image_tag != '' }}
      - name: Build & push release image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: Dockerfile.api
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

